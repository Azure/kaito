training_config:
  ModelConfig: # Passed in https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/modeling_utils.py#L2598
    torch_dtype: "bfloat16"  # Model Precision
    local_files_only: True   # Local model weights already present
    device_map: "auto"

  TokenizerParams: # Passed in https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/tokenization_utils_base.py#L2798
    padding: True
    truncation: True

  QuantizationConfig: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/utils/quantization_config.py#L179
    load_in_4bit: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: True

  LoraConfig: # Uses https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L45
    r: 16
    lora_alpha: 32
    target_modules: "query_key_value"
    lora_dropout: 0.05
    bias: "none"

  TrainingArguments: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/training_args.py#L167
    output_dir: "." # Path where training results are saved
    num_train_epochs: 4
    auto_find_batch_size: True
    ddp_find_unused_parameters: False # Disabled to prevent distributed training errors from conditional params
    save_strategy: "epoch"

  DatasetConfig: # Custom Implementation
    shuffle_dataset: True
    train_test_split: 1  # Use all data for training

  DataCollator: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/data/data_collator.py#L634
    mlm: False # False for tasks/models not using masked language modeling, like Falcon
