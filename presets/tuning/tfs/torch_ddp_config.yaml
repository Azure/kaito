# Depending on model, would require additional param: --ddp_find_unused_parameters=False
accelerate_config: # Remove place in command line
  compute_environment: LOCAL_MACHINE
  debug: false
  distributed_type: MULTI_GPU
  downcast_bf16: 'no'
  gpu_ids: all
  machine_rank: 0
  main_training_function: main
  mixed_precision: 'no'
  num_machines: 1
  num_processes: 2
  rdzv_backend: static
  same_network: true
  tpu_env: []
  tpu_use_cluster: false
  tpu_use_sudo: false
  use_cpu: false

training_config:
  ModelConfig: # Passed in https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/modeling_utils.py#L2598
    torch_dtype: "bfloat16"  # Model Precision
    local_files_only: True  # Local model weights present
    device_map: "auto"

  TokenizerParams: # Passed in https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/tokenization_utils_base.py#L2798
    padding: True
    truncation: True

  QuantizationConfig: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/utils/quantization_config.py#L179
    load_in_4bit: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: True

  LoraConfig: # Uses https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L45
    r: 16
    lora_alpha: 32
    target_modules: "query_key_value"
    lora_dropout: 0.05
    bias: "none"

  TrainingArguments: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/training_args.py#L167
    output_dir: "." # Path where training results are saved
    num_train_epochs: 4
    auto_find_batch_size: True
    ddp_find_unused_parameters: False # Disabled to prevent distributed training errors from conditional params
    save_strategy: "epoch"

  DatasetConfig: # Custom Implementation
    dataset_name: "Amod/mental_health_counseling_conversations" # TODO: Get rid of, standardize with /data-volume
    shuffle_dataset: True
    train_test_split: 1  # Use all data for training

  DataCollator: # Uses https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/data/data_collator.py#L634
    mlm: False # False for tasks/models not using masked language modeling, like Falcon

# Way to update params is new configmap name in CRD
# Controller deploys the configmap
# Ensure configmap exists before pod starts - podstartcheck
# cmd line parameters override configmap params
# TODO: Validation Schema Check
# TODO: Documentation OpenAPI Spec

